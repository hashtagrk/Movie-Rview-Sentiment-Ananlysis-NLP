{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# BeautifulSoup is used to remove html tags from the text\n",
    "from bs4 import BeautifulSoup \n",
    "import re # For regular expressions\n",
    "\n",
    "# Stopwords can be useful to undersand the semantics of the sentence.\n",
    "# Therefore stopwords are not removed while creating the word2vec model.\n",
    "# But they will be removed  while averaging feature vectors.\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./labeledTrainData.tsv\", header=0,\\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "test = pd.read_csv(\"./testData.tsv\",header=0,\\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_wordlist(review, remove_stopwords=True):\n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "#nltk.download('popular')\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # 1. Using nltk tokenizer\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_wordlist(raw_sentence,\\\n",
    "                                            remove_stopwords))\n",
    "\n",
    "    # This returns the list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaCondaInstaller\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\anaCondaInstaller\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\anaCondaInstaller\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_sentences(review, tokenizer)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 00:54:02,929 : INFO : collecting all words and their counts\n",
      "2019-03-09 00:54:02,929 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 00:54:02,992 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2019-03-09 00:54:03,051 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2019-03-09 00:54:03,113 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2019-03-09 00:54:03,174 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2019-03-09 00:54:03,235 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2019-03-09 00:54:03,293 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2019-03-09 00:54:03,328 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2019-03-09 00:54:03,405 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2019-03-09 00:54:03,456 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2019-03-09 00:54:03,517 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2019-03-09 00:54:03,577 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2019-03-09 00:54:03,639 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2019-03-09 00:54:03,700 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2019-03-09 00:54:03,759 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2019-03-09 00:54:03,819 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2019-03-09 00:54:03,873 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2019-03-09 00:54:03,931 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2019-03-09 00:54:03,995 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2019-03-09 00:54:04,059 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2019-03-09 00:54:04,119 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2019-03-09 00:54:04,179 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2019-03-09 00:54:04,227 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2019-03-09 00:54:04,292 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2019-03-09 00:54:04,347 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2019-03-09 00:54:04,415 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2019-03-09 00:54:04,465 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2019-03-09 00:54:04,508 : INFO : collected 74218 word types from a corpus of 5920724 raw words and 266551 sentences\n",
      "2019-03-09 00:54:04,509 : INFO : Loading a fresh vocabulary\n",
      "2019-03-09 00:54:04,572 : INFO : effective_min_count=20 retains 13153 unique words (17% of original 74218, drops 61065)\n",
      "2019-03-09 00:54:04,573 : INFO : effective_min_count=20 leaves 5694846 word corpus (96% of original 5920724, drops 225878)\n",
      "2019-03-09 00:54:04,630 : INFO : deleting the raw counts dictionary of 74218 items\n",
      "2019-03-09 00:54:04,633 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2019-03-09 00:54:04,634 : INFO : downsampling leaves estimated 4195316 word corpus (73.7% of prior 5694846)\n",
      "2019-03-09 00:54:04,692 : INFO : estimated required memory for 13153 words and 300 dimensions: 38143700 bytes\n",
      "2019-03-09 00:54:04,693 : INFO : resetting layer weights\n",
      "2019-03-09 00:54:04,898 : INFO : training model with 4 workers on 13153 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-09 00:54:05,927 : INFO : EPOCH 1 - PROGRESS: at 15.50% examples, 653197 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:06,934 : INFO : EPOCH 1 - PROGRESS: at 31.60% examples, 660307 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:07,935 : INFO : EPOCH 1 - PROGRESS: at 47.45% examples, 661984 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:08,941 : INFO : EPOCH 1 - PROGRESS: at 63.54% examples, 663464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:09,937 : INFO : EPOCH 1 - PROGRESS: at 79.47% examples, 664670 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:10,945 : INFO : EPOCH 1 - PROGRESS: at 93.63% examples, 652848 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:11,355 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-09 00:54:11,355 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 00:54:11,373 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 00:54:11,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 00:54:11,385 : INFO : EPOCH - 1 : training on 5920724 raw words (4195167 effective words) took 6.5s, 648886 effective words/s\n",
      "2019-03-09 00:54:12,397 : INFO : EPOCH 2 - PROGRESS: at 15.50% examples, 648486 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:13,414 : INFO : EPOCH 2 - PROGRESS: at 31.42% examples, 655087 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:14,437 : INFO : EPOCH 2 - PROGRESS: at 47.61% examples, 657944 words/s, in_qsize 6, out_qsize 1\n",
      "2019-03-09 00:54:15,439 : INFO : EPOCH 2 - PROGRESS: at 63.71% examples, 659498 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-09 00:54:16,454 : INFO : EPOCH 2 - PROGRESS: at 77.81% examples, 645966 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:17,456 : INFO : EPOCH 2 - PROGRESS: at 91.74% examples, 636049 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:17,931 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-09 00:54:17,931 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 00:54:17,947 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 00:54:17,955 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 00:54:17,956 : INFO : EPOCH - 2 : training on 5920724 raw words (4194595 effective words) took 6.6s, 639282 effective words/s\n",
      "2019-03-09 00:54:18,977 : INFO : EPOCH 3 - PROGRESS: at 15.50% examples, 653909 words/s, in_qsize 8, out_qsize 1\n",
      "2019-03-09 00:54:19,974 : INFO : EPOCH 3 - PROGRESS: at 31.77% examples, 663726 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:20,988 : INFO : EPOCH 3 - PROGRESS: at 47.61% examples, 661872 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:21,988 : INFO : EPOCH 3 - PROGRESS: at 63.87% examples, 665344 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:23,015 : INFO : EPOCH 3 - PROGRESS: at 80.32% examples, 668389 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:24,024 : INFO : EPOCH 3 - PROGRESS: at 96.70% examples, 670414 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:24,192 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-09 00:54:24,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 00:54:24,211 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 00:54:24,223 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 00:54:24,224 : INFO : EPOCH - 3 : training on 5920724 raw words (4194126 effective words) took 6.3s, 670626 effective words/s\n",
      "2019-03-09 00:54:25,243 : INFO : EPOCH 4 - PROGRESS: at 15.66% examples, 658871 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:26,231 : INFO : EPOCH 4 - PROGRESS: at 31.77% examples, 665121 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:27,236 : INFO : EPOCH 4 - PROGRESS: at 47.94% examples, 670017 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:28,248 : INFO : EPOCH 4 - PROGRESS: at 64.38% examples, 673539 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:29,255 : INFO : EPOCH 4 - PROGRESS: at 80.49% examples, 673641 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:30,245 : INFO : EPOCH 4 - PROGRESS: at 96.36% examples, 671870 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:30,464 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 00:54:30,465 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 00:54:30,478 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 00:54:30,487 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 00:54:30,488 : INFO : EPOCH - 4 : training on 5920724 raw words (4195290 effective words) took 6.3s, 670792 effective words/s\n",
      "2019-03-09 00:54:31,492 : INFO : EPOCH 5 - PROGRESS: at 15.34% examples, 650730 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:32,524 : INFO : EPOCH 5 - PROGRESS: at 29.44% examples, 612873 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:33,530 : INFO : EPOCH 5 - PROGRESS: at 43.13% examples, 598764 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:34,538 : INFO : EPOCH 5 - PROGRESS: at 58.82% examples, 612147 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-09 00:54:35,554 : INFO : EPOCH 5 - PROGRESS: at 74.76% examples, 621726 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:36,564 : INFO : EPOCH 5 - PROGRESS: at 89.97% examples, 624508 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-09 00:54:37,222 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-09 00:54:37,236 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-09 00:54:37,244 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-09 00:54:37,252 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-09 00:54:37,253 : INFO : EPOCH - 5 : training on 5920724 raw words (4194806 effective words) took 6.7s, 621529 effective words/s\n",
      "2019-03-09 00:54:37,254 : INFO : training on a 29603620 raw words (20973984 effective words) took 32.3s, 648508 effective words/s\n",
      "2019-03-09 00:54:37,264 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-03-09 00:54:37,421 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-03-09 00:54:37,421 : INFO : not storing attribute vectors_norm\n",
      "2019-03-09 00:54:37,421 : INFO : not storing attribute cum_table\n",
      "2019-03-09 00:54:38,054 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 20 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                          workers=num_workers,\n",
    "                          size=num_features,\n",
    "                          min_count=min_word_count,\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaCondaInstaller\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('horrible', 0.9171717166900635),\n",
       " ('awful', 0.8227857351303101),\n",
       " ('atrocious', 0.7224851846694946),\n",
       " ('laughable', 0.7200475931167603),\n",
       " ('dreadful', 0.7119424939155579),\n",
       " ('horrid', 0.6983188390731812),\n",
       " ('bad', 0.687454104423523),\n",
       " ('lame', 0.6834261417388916),\n",
       " ('pathetic', 0.6772339940071106),\n",
       " ('abysmal', 0.6757860779762268)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"terrible\")\n",
    "#https://cs224d.stanford.edu/reports/PouransariHadi.pdf\n",
    "#https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaCondaInstaller\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'mj']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "words[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.46628879e-02  1.07706273e-02  1.13535523e-01 -1.09947158e-03\n",
      "  4.55297641e-02 -1.09991767e-02  8.97706859e-03  3.06147505e-02\n",
      "  2.74838344e-03 -1.79987215e-02 -1.57652311e-02  3.54621448e-02\n",
      " -1.88522246e-02  8.42513330e-03  3.03037558e-02 -5.34946285e-02\n",
      " -5.84037751e-02  5.32212807e-03  5.08216396e-03 -1.71942972e-02\n",
      "  4.68900427e-02  1.06043788e-02  5.38697727e-02  2.86109652e-02\n",
      " -7.74646029e-02 -2.94330381e-02 -1.32919699e-01 -3.40857692e-02\n",
      "  1.08589204e-02 -9.05710608e-02  6.83899373e-02 -7.74414837e-02\n",
      "  9.52017084e-02  3.23455445e-02  2.45676506e-02  1.75810251e-02\n",
      " -2.80611720e-02  8.95487294e-02  1.63848817e-01 -1.95380952e-03\n",
      "  1.27040327e-01  1.66469708e-01  1.91589966e-02 -2.60981880e-02\n",
      " -2.98508047e-03  2.01526359e-02  5.97073585e-02  4.96000908e-02\n",
      "  4.30821348e-03 -3.96617614e-02  3.15747336e-02 -1.00123614e-01\n",
      "  2.27299724e-02  7.28075877e-02  5.82443625e-02 -8.40770304e-02\n",
      "  7.34475926e-02  1.52407568e-02  1.12994071e-02  1.53573751e-02\n",
      "  6.56339228e-02  3.11040378e-04  4.89097903e-04  1.94555409e-02\n",
      " -3.60542834e-02  3.69594283e-02 -5.34802452e-02  9.21653286e-02\n",
      " -3.24651711e-02 -5.67557178e-02 -6.41779676e-02 -5.68752773e-02\n",
      " -2.08288785e-02  3.73962596e-02 -2.94632334e-02 -1.43252090e-01\n",
      " -7.99672399e-03  3.49562019e-02  4.52394262e-02 -9.07432809e-02\n",
      " -3.63955013e-02 -7.50564858e-02  7.94976801e-02 -1.00981064e-01\n",
      " -4.56123017e-02 -5.91147952e-02  3.22673507e-02  5.36328531e-04\n",
      " -5.17811701e-02  3.60996299e-03 -1.23690525e-02  2.09546760e-02\n",
      " -5.13097644e-02 -9.49210152e-02 -2.86764521e-02 -1.96560100e-02\n",
      "  9.77252871e-02 -1.69172231e-02  3.98929417e-02  6.19412586e-02\n",
      " -1.49605321e-02 -6.42039394e-03  2.55106725e-02 -1.87509954e-02\n",
      " -9.08567235e-02  1.08817900e-02  1.07260339e-01  4.54205051e-02\n",
      "  6.54890463e-02  7.88799897e-02  1.56967938e-02 -3.41951102e-02\n",
      "  2.07011662e-02 -9.31747332e-02  1.84517391e-02 -3.75854746e-02\n",
      " -1.83360204e-02  7.07929730e-02 -3.70992091e-03  1.21081501e-01\n",
      "  8.36981907e-02 -8.24250840e-03  3.59612405e-02 -7.81306475e-02\n",
      " -5.97256832e-02 -1.39231846e-01 -1.88306998e-02 -6.21116301e-03\n",
      " -2.50739790e-02 -3.59660275e-02 -2.65046526e-02 -8.48582294e-03\n",
      "  1.33204143e-02  9.24965134e-05 -6.20399676e-02  1.03792571e-01\n",
      "  3.76770869e-02  5.18840812e-02 -7.62087852e-02  1.27006928e-02\n",
      "  6.83491006e-02 -8.25715810e-02 -3.24358828e-02  3.04280017e-02\n",
      "  2.08547525e-02  7.93302283e-02 -4.94961701e-02  6.19882494e-02\n",
      "  2.43166443e-02 -3.08487844e-02 -2.05227565e-02  9.13384557e-02\n",
      "  1.03030868e-01 -5.09230979e-02  1.09221600e-01 -6.04599677e-02\n",
      "  1.29093407e-02  5.98659553e-02  2.30449382e-02  3.25048342e-02\n",
      " -5.35520911e-02 -2.36932002e-02  4.05493751e-02 -7.84379765e-02\n",
      "  1.19407615e-03 -1.56351104e-02 -5.30788377e-02  1.94638630e-03\n",
      " -1.03040963e-01 -9.12524387e-03 -3.27193365e-02  1.70626178e-01\n",
      "  1.16000688e-02  2.79322788e-02 -9.65004712e-02 -4.01724167e-02\n",
      " -4.02015746e-02 -7.44784623e-02  8.55588634e-03 -5.13886958e-02\n",
      "  7.81021938e-02  3.10072899e-02 -5.39710522e-02 -8.49013329e-02\n",
      "  7.17446283e-02  4.72996421e-02  8.79654288e-02  2.79376488e-02\n",
      "  2.17472725e-02  5.72545864e-02 -9.83520597e-02  4.09848988e-02\n",
      "  9.12068635e-02  3.01771238e-02 -3.06898598e-02  3.47840860e-02\n",
      "  2.53290106e-02  5.96543476e-02 -4.89175059e-02  1.53286671e-02\n",
      " -5.31156622e-02 -7.79042691e-02  7.02702850e-02 -1.47155588e-02\n",
      " -5.72439581e-02 -1.21334851e-01 -1.00701107e-02  3.74230444e-02\n",
      " -4.94391359e-02  1.30373230e-02 -5.43803908e-02  1.62082596e-03\n",
      "  3.59506123e-02 -1.22024052e-01 -4.32472117e-03  4.88105379e-02\n",
      "  5.76206930e-02 -3.64297442e-02 -4.53251265e-02 -7.96089023e-02\n",
      " -4.91808616e-02  1.88554674e-02 -1.38644001e-03 -7.95425400e-02\n",
      "  4.91792932e-02 -3.33381929e-02  7.77718648e-02  6.01088181e-02\n",
      "  3.49402241e-02 -7.42899766e-03  9.11173075e-02 -1.98616702e-02\n",
      " -3.59135270e-02 -5.25031388e-02 -1.87043585e-02  5.82276145e-03\n",
      " -1.86079200e-02  3.76769342e-02  2.37134602e-02  3.68500464e-02\n",
      " -2.74313577e-02  7.23192617e-02  1.38831869e-01 -1.59836933e-01\n",
      " -9.55973119e-02 -4.14851755e-02  4.54940088e-02  4.76270542e-02\n",
      "  8.43375269e-03 -1.11669257e-01 -6.54654985e-04 -4.96807955e-02\n",
      "  7.39516644e-03 -1.63030401e-02 -1.07478507e-01 -6.68810308e-02\n",
      " -5.76076377e-03 -4.75344136e-02  2.82727797e-02  4.59284671e-02\n",
      "  6.81094602e-02  8.00276026e-02 -1.36043830e-02  2.18014326e-03\n",
      "  4.15478051e-02  4.97017615e-02 -1.01107182e-02  8.00346583e-02\n",
      " -2.97336951e-02  1.06282428e-01 -5.67853898e-02 -6.43767118e-02\n",
      "  2.63924744e-06 -2.91708931e-02 -2.65866611e-02  6.16720319e-02\n",
      "  6.30215481e-02 -3.50417718e-02 -3.78809869e-02  3.81067372e-03\n",
      " -3.91742140e-02 -2.93229874e-02 -1.68658290e-02  3.65813896e-02\n",
      " -1.64091587e-02  2.88823489e-02  1.04218945e-01 -4.83570471e-02\n",
      " -7.28165731e-02 -1.39650060e-02  3.79586685e-03 -3.65698594e-03\n",
      "  4.96346615e-02  1.10610008e-01 -1.68300513e-02 -7.69853368e-02\n",
      " -1.22630941e-02 -5.40333427e-02  3.06629781e-02  4.40046228e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaCondaInstaller\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['with'])\n",
    "#This is how 300 dimensional word2vec look for the word 'with'. Try with different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mahe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaCondaInstaller\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=True))\n",
    "    \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaCondaInstaller\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_wordlist(review,remove_stopwords=True))\n",
    "    \n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaCondaInstaller\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "clean_custom_test_reviews = []\n",
    "custom_snt = 'one of the worst movie ever'\n",
    "clean_custom_test_reviews.append(review_wordlist(custom_snt,remove_stopwords=True))\n",
    "testDataCustomVecs = getAvgFeatureVecs(clean_custom_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(trainDataVecs,train[\"sentiment\"],test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting random forest to training data....\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "    \n",
    "print(\"Fitting random forest to training data....\")    \n",
    "forest = forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n",
      "0.8356\n",
      "0.8552950687146321\n",
      "[[3093  696]\n",
      " [ 537 3174]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,confusion_matrix\n",
    "y_pred = forest.predict(X_test)\n",
    "print(len(X_test))\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(recall_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        review  sentiment\n",
      "0  one of the worst movie ever          0\n"
     ]
    }
   ],
   "source": [
    "result1 = forest.predict(testDataCustomVecs)\n",
    "output = pd.DataFrame(data={\"review\":custom_snt, \"sentiment\":result1})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Review  sentiment\n",
      "0      \"Naturally in a film who's main themes are of ...          1\n",
      "1      \"This movie is a disaster within a disaster fi...          0\n",
      "2      \"All in all, this is a movie for kids. We saw ...          1\n",
      "3      \"Afraid of the Dark left me with the impressio...          0\n",
      "4      \"A very accurate depiction of small time mob l...          1\n",
      "5      \"...as valuable as King Tut's tomb! (OK, maybe...          1\n",
      "6      \"This has to be one of the biggest misfires ev...          0\n",
      "7      \"This is one of those movies I watched, and wo...          0\n",
      "8      \"The worst movie i've seen in years (and i've ...          0\n",
      "9      \"Five medical students (Kevin Bacon, David Lab...          1\n",
      "10     \"'The Mill on the Floss' was one of the lesser...          1\n",
      "11     \"I just saw this film at the phoenix film fest...          0\n",
      "12     \"\\\"The Love Letter\\\" is one of those movies th...          0\n",
      "13     \"Another fantastic offering from the Monkey Is...          1\n",
      "14     \"This was included on the disk \\\"Shorts: Volum...          0\n",
      "15     \"I'm not really much of an Abbott & Costello f...          1\n",
      "16     \"This movie was dreadful. Biblically very inac...          1\n",
      "17     \"I don't think I've ever gave something a 1/10...          0\n",
      "18     \"Excellent story-telling and cinematography. P...          1\n",
      "19     \"I completely forgot that I'd seen this within...          0\n",
      "20     \"I like action movies. I have a softspot for \\...          0\n",
      "21     \"This is one of the worst Sandra Bullock movie...          0\n",
      "22     \"Watched this flick on Saturday afternoon cabl...          0\n",
      "23     \"I went to see \\\"TKIA\\\" with high expectations...          0\n",
      "24     \"All credit to writer/director Gilles Mimouni ...          1\n",
      "25     \"As a writing teacher, there are two ending I ...          0\n",
      "26     \"I don't know why this has gotten any decent r...          0\n",
      "27     \"This film was released in the UK under the na...          0\n",
      "28     \"Uncle Fred Olen Ray once again gives us a lit...          0\n",
      "29     \"OK, it's watchable if you are sick in bed or ...          0\n",
      "...                                                  ...        ...\n",
      "24970  \"With \\\"Anatomy\\\" the german film producers ha...          1\n",
      "24971  \"This movie is one of my all-time favorites. I...          1\n",
      "24972  \"I found Code 46 very disappointing. I thought...          0\n",
      "24973  \"Tamara Anderson and her family are moving onc...          0\n",
      "24974  \"Now I've seen it all. Just when I thought it ...          0\n",
      "24975  \"In this movie everything possible was wrong a...          0\n",
      "24976  \"Well every scene so perfectly presented. Neve...          1\n",
      "24977  \"Sleeper Cell is what 24 should have been. 24 ...          0\n",
      "24978  \"Not for everyone, but I really like it. Nice ...          1\n",
      "24979  \"Set just before the Second World War, this is...          1\n",
      "24980  \"Contains Spoiler The movie is a good action/c...          0\n",
      "24981  \"This is one of several period sea-faring yarn...          1\n",
      "24982  \"Hearkening back to those \\\"Good Old Days\\\" of...          1\n",
      "24983  \"I thought this to be a pretty good example of...          0\n",
      "24984  \"Seeing this film, or rather set of films, in ...          1\n",
      "24985  \"I didn't like this movie for many reasons - V...          0\n",
      "24986  \"I absolutely love this show!!!!!!!, Its basic...          1\n",
      "24987  \"eXistenZ combines director David Cronenberg's...          0\n",
      "24988  \"this movie is allegedly a comedy.so where did...          0\n",
      "24989  \"The Comebacks is a spoof on inspirational spo...          1\n",
      "24990  \"I'd love to write a little summary of this mo...          0\n",
      "24991  \"Obvious tailored vehicle for Ryan Philippe. I...          0\n",
      "24992  \"<br /><br />JURASSIC PARK III *___ Adventure ...          0\n",
      "24993  \"If you're even mildly interested in the War b...          1\n",
      "24994  \"It used to be that video distributors like Su...          0\n",
      "24995  \"Sony Pictures Classics, I'm looking at you! S...          1\n",
      "24996  \"I always felt that Ms. Merkerson had never go...          1\n",
      "24997  \"I was so disappointed in this movie. I am ver...          1\n",
      "24998  \"From the opening sequence, filled with black ...          1\n",
      "24999  \"This is a great horror film for people who do...          0\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "result = forest.predict(testDataVecs)\n",
    "#output = pd.DataFrame(data={\"review\":test[\"review\"], \"sentiment\":result})\n",
    "output = pd.DataFrame(data={\"review\":test[\"review\"], \"sentiment\":result})\n",
    "print(output)\n",
    "#output.to_csv( \"output.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
